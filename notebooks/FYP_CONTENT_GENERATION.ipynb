{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git add .\n"
      ],
      "metadata": {
        "id": "bkh3OvOuQHiT",
        "outputId": "3db0040d-acee-445d-8b49-1a7c5bdb23e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### 21i1667 , 21i1665 , 21i1662"
      ],
      "metadata": {
        "id": "CksT2KomP6Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYpmmfQ-ZbIh",
        "outputId": "342fc5f0-790e-46be-e001-eb4da170786f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.0/408.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.9/296.9 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain openai  -q  #Run To install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYfy2IhtvGVv",
        "outputId": "9d11b17b-a74d-43d0-bc9a-b19e58153040",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/255.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/255.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbL5zdf9aG8a",
        "outputId": "fa2984a5-701e-46dc-d4bd-61ec7baefd49",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/981.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.5/488.5 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m989.0/989.0 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m327.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git (to revision v0.6) to /tmp/pip-install-0vy1w7au/detectron2_538d623a41184e079281f0485191cacc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-install-0vy1w7au/detectron2_538d623a41184e079281f0485191cacc\n",
            "  Running command git checkout -q d1e04565d3bec8719335b88be9e9b961bf3ec464\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit d1e04565d3bec8719335b88be9e9b961bf3ec464\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (10.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (2.5.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (3.1.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (4.66.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (2.17.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (1.0.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (3.0.2)\n",
            "Requirement already satisfied: omegaconf>=2.1 in /usr/local/lib/python3.10/dist-packages (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (2.3.0)\n",
            "Collecting hydra-core>=1.1 (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black==21.4b2 (from detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2)\n",
            "  Downloading black-21.4b2-py3-none-any.whl.metadata (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from black==21.4b2->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (8.1.7)\n",
            "Collecting appdirs (from black==21.4b2->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from black==21.4b2->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (0.10.2)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.10/dist-packages (from black==21.4b2->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (2024.9.11)\n",
            "Collecting pathspec<1,>=0.8.1 (from black==21.4b2->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black==21.4b2->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (6.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (24.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (2.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (2.8.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2) (3.0.2)\n",
            "Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=6338385 sha256=5d07ab985a4fd1367fb569d9a119a7f858ab4e0e9375ca6c760912d06811b10b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ox_tb1nx/wheels/d0/62/a4/633b274d7706eff61ae574ae90fca694eeb99efbc2d719069f\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=2886f7ca8434a5385151bd85f4b0fd1a8db4c495fbeddc34ab48f352f5d93584\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "Successfully built detectron2 fvcore\n",
            "Installing collected packages: appdirs, yacs, pathspec, iopath, hydra-core, fvcore, black, detectron2\n",
            "  Attempting uninstall: iopath\n",
            "    Found existing installation: iopath 0.1.10\n",
            "    Uninstalling iopath-0.1.10:\n",
            "      Successfully uninstalled iopath-0.1.10\n",
            "Successfully installed appdirs-1.4.4 black-21.4b2 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 pathspec-0.12.1 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install unstructured -q\n",
        "!pip install unstructured[local-inference] -q\n",
        "!pip install detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwalYTVZoRlH",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a53f787-4b1c-4a18-8ca2-731a6bc56d01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 0s (747 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123622 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AssnWUVlxtH"
      },
      "source": [
        "https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/directory_loader.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3uUDJ179DSH",
        "outputId": "6dccdce1-378d-4dd3-a266-3e097470513a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 0s (16.5 MB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123652 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y tesseract-ocr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwaCosqsogzw"
      },
      "source": [
        "https://python.langchain.com/en/latest/modules/indexes/text_splitters/getting_started.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrvV0RDWLGyF",
        "outputId": "0713a422-7615-4b79-dc2c-c57a43abd0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/2.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community -q   #Run TILL HERE Then go down and run the embeddings I have inserted comment there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqwKly8lTqzl",
        "outputId": "52d4a5ec-448f-421d-b42d-069e38b769d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pypdf._cmap:Advanced encoding [] not implemented yet\n",
            "ERROR:pypdf._cmap:Advanced encoding [] not implemented yet\n",
            "ERROR:pypdf._cmap:Advanced encoding [] not implemented yet\n",
            "ERROR:pypdf._cmap:Advanced encoding [] not implemented yet\n",
            "ERROR:pypdf._cmap:Advanced encoding [] not implemented yet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents loaded: 7007\n",
            "Number of document chunks created: 35980\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "directory = '/content/data'\n",
        "\n",
        "def load_docs(directory):\n",
        "    # Create a list to hold all the loaded documents\n",
        "    documents = []\n",
        "\n",
        "    # Iterate over each file in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.pdf'):\n",
        "            # Construct the full file path\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            # Load the PDF file\n",
        "            pdf_loader = PyPDFLoader(file_path)\n",
        "            documents.extend(pdf_loader.load())  # Append the loaded documents\n",
        "\n",
        "    return documents\n",
        "\n",
        "def split_docs(documents, chunk_size=500, chunk_overlap=20):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    return docs\n",
        "\n",
        "# Load documents from the specified directory\n",
        "documents = load_docs(directory)\n",
        "print(f\"Number of documents loaded: {len(documents)}\")\n",
        "\n",
        "# Split the loaded documents into chunks\n",
        "docs = split_docs(documents)\n",
        "print(f\"Number of document chunks created: {len(docs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Suppress pypdf error logs if desired\n",
        "logging.getLogger(\"pypdf\").setLevel(logging.WARNING)\n",
        "\n",
        "directory = '/content/data'\n",
        "\n",
        "def load_docs(directory):\n",
        "    documents = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.pdf'):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            try:\n",
        "                pdf_loader = PyMuPDFLoader(file_path)\n",
        "                loaded_docs = pdf_loader.load()\n",
        "                documents.extend(loaded_docs)\n",
        "                print(f\"Loaded {len(loaded_docs)} pages from {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load {file_path}: {e}\")\n",
        "    return documents\n",
        "\n",
        "def split_docs(documents, chunk_size=500, chunk_overlap=20):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return text_splitter.split_documents(documents)\n",
        "\n",
        "# Load documents from the specified directory\n",
        "documents = load_docs(directory)\n",
        "print(f\"Total number of documents loaded: {len(documents)}\")\n",
        "\n",
        "# Split the loaded documents into chunks\n",
        "docs = split_docs(documents)\n",
        "print(f\"Total number of document chunks created: {len(docs)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ78iJ3fWWdV",
        "outputId": "aef38db3-93bc-4ef6-a3c4-3af991c8c705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1270 pages from Book - Tony Gaddis - Starting Out with C++.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 0 of document /content/data/Data Mining Practical Machine Learning Tools and Techniques 3rd Edition-Manteshbbbb.pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 665 pages from Data Mining Practical Machine Learning Tools and Techniques 3rd Edition-Manteshbbbb.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 559 of document /content/data/Database_Systems.pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1029 pages from Database_Systems.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 0 of document /content/data/Data Structures 1.pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 738 pages from Data Structures 1.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 2 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 12 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 20 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 499 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 787 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 1161 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 1250 of document /content/data/Algo Book.pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1313 pages from Algo Book.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 0 of document /content/data/Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville (z-lib.org).pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 801 pages from Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville (z-lib.org).pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 1 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 11 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 29 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 85 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 139 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 141 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 161 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 273 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 299 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 301 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 395 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 419 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 491 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 501 of document /content/data/eisenstein-natural-language-processing.pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 587 pages from eisenstein-natural-language-processing.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 40 of document /content/data/Data Mining and Analysis by Zaki.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 224 of document /content/data/Data Mining and Analysis by Zaki.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 338 of document /content/data/Data Mining and Analysis by Zaki.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 340 of document /content/data/Data Mining and Analysis by Zaki.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 473 of document /content/data/Data Mining and Analysis by Zaki.pdf\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/parsers/pdf.py:299: UserWarning: Warning: Empty content on page 591 of document /content/data/Data Mining and Analysis by Zaki.pdf\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 604 pages from Data Mining and Analysis by Zaki.pdf\n",
            "Total number of documents loaded: 7007\n",
            "Total number of document chunks created: 35122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t54HueEUW6-Q",
        "outputId": "1e6a7bb2-c140-4776-8d5c-4cac76f0e258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Document 1:\n",
            "Credits and acknowledgments borrowed from other sources and reproduced, with permission, appear on the \n",
            "Credits page in the endmatter of this textbook.\n",
            "Copyright © 2015, 2012, 2009 Pearson Education, Inc., publishing as Addison-Wesley All rights reserved. \n",
            "Manufactured in the United States of America. This publication is protected by Copyright, and permission \n",
            "should be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 2:\n",
            "transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. To \n",
            "obtain permission(s) to use material from this work, please submit a written request to Pearson Education, \n",
            "Inc., Permissions Department, One Lake Street, Upper Saddle River, New Jersey 07458 or you may fax your \n",
            "request to 201 236-3290.\n",
            "Many of the designations by manufacturers and sellers to distinguish their products are claimed as trademarks.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 3:\n",
            "Where those designations appear in this book, and the publisher was aware of a trademark claim, the \n",
            "designations have been printed in initial caps or all caps.\n",
            "Library of Congress Cataloging-in-Publication Data\n",
            "Gaddis, Tony.\n",
            " Starting out with C++ : from control structures through objects/Tony Gaddis.—Eighth edition.\n",
            "  pages cm\n",
            " Includes bibliographical references and index.\n",
            " Online the following appendices are available at www.pearsonhighered.com/gaddis: Appendix D:\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 4:\n",
            "Introduction to fl  owcharting; Appendix E: Using UML in class design; Appendix F: Namespaces; Appendix G: \n",
            "Writing managed C++ code for the .net framework; Appendix H: Passing command line arguments; Appendix \n",
            "I: Header fi  le and library function reference; Appendix J: Binary numbers and bitwise operations; Appendix K: \n",
            "Multi-source fi  le programs; Appendix L: Stream member functions for formatting; Appendix M: Introduction\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 5:\n",
            "to Microsoft Visual C++ 2010 express edition; Appendix N: Answers to checkpoints; and Appendix O: \n",
            "Solutions to odd-numbered review questions.\n",
            " ISBN-13: 978-0-13-376939-5\n",
            " ISBN-10: 0-13-376939-9\n",
            " 1. C++ (Computer program language)  I. Title.  II. Title: From control structures through objects.\n",
            "  QA76.73.C153G33 2014b\n",
            "  005.13’3—dc23\n",
            "                                                            2014000213\n",
            "10  9  8  7  6  5  4  3  2  1   Editorial Director: Marcia Horton\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the first 5 cleaned document chunks\n",
        "for i, doc in enumerate(docs[5:10]):\n",
        "    print(f\"Cleaned Document {i + 1}:\")\n",
        "    print(doc.page_content)\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGCbi4niFSVX",
        "outputId": "fc750340-b477-4957-db26-5aaa170f237e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of cleaned document chunks: 35980\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans a single text string by performing various cleaning steps,\n",
        "    including removing page numbers, headers, and footers.\n",
        "    \"\"\"\n",
        "    # Remove extra whitespaces, tabs, and newlines\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove patterns that typically indicate page numbers, e.g., \"Page 1\", \"1 of 10\", etc.\n",
        "    text = re.sub(r'\\bpage\\s*\\d+\\b', '', text, flags=re.IGNORECASE)  # Matches \"Page 1\"\n",
        "    text = re.sub(r'\\b\\d+\\s*(of|/)\\s*\\d+\\b', '', text, flags=re.IGNORECASE)  # Matches \"1 of 10\" or \"1/10\"\n",
        "\n",
        "    # Remove common header/footer indicators (customize this part based on your documents)\n",
        "    text = re.sub(r'(header|footer|chapter\\s*\\d+|section\\s*\\d+)', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove any lines that are too short (likely headers/footers, e.g., under 5 characters)\n",
        "    text = re.sub(r'\\b\\w{1,4}\\b', '', text)\n",
        "\n",
        "    # Remove special characters (except common punctuation marks)\n",
        "    text = re.sub(r'[^\\w\\s.,?!]', '', text)\n",
        "\n",
        "    # Convert to lowercase (optional, depending on use case)\n",
        "    #text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_documents(documents):\n",
        "    \"\"\"\n",
        "    Cleans the text for each document in the list by removing headers, footers, and other noise.\n",
        "    \"\"\"\n",
        "    cleaned_docs = []\n",
        "    for doc in documents:\n",
        "        # Clean the content of the document\n",
        "        cleaned_content = clean_text(doc.page_content)\n",
        "        # Update the cleaned content in the document\n",
        "        doc.page_content = cleaned_content\n",
        "        cleaned_docs.append(doc)\n",
        "\n",
        "    return cleaned_docs\n",
        "\n",
        "# Clean the loaded document chunks\n",
        "cleaned_docs = clean_documents(docs)\n",
        "print(f\"Number of cleaned document chunks: {len(cleaned_docs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PiPwt-FaYwl",
        "outputId": "c59f3051-543d-48c8-ec7d-d99a9d4df569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Document 1:\n",
            "Starting    Eighth .  Gaddis Computer Science  University  Texas  Austin  . Document shared  .docsity.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 2:\n",
            "Document shared  .docsity.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 3:\n",
            " EIGHTH EDITION STARTING     Control Structures through Objects Document shared  .docsity.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 4:\n",
            "  intentionally  blank Document shared  .docsity.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Cleaned Document 5:\n",
            " EIGHTH EDITION STARTING     Control Structures through Objects  Gaddis Haywood Community College Boston Columbus Indianapolis    Francisco Upper Saddle River Amsterdam   Dubai London Madrid Milan Munich Paris Montreal Toronto Delhi Mexico   Paulo Sydney   Seoul Singapore Taipei Tokyo Document shared  .docsity.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the first 5 cleaned document chunks\n",
        "for i, doc in enumerate(cleaned_docs[:5]):\n",
        "    print(f\"Cleaned Document {i + 1}:\")\n",
        "    print(doc.page_content)\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F5GY9voPa0av",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "82a5f293-923c-410c-bbf8-972db190b254"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-899aae643cf6>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from langchain.embeddings.openai import OpenAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# embeddings = OpenAIEmbeddings(model_name=\"ada\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformerEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformerEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"all-MiniLM-L6-v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "# Now run start running from here and go till end\n",
        "\n",
        "\n",
        "\n",
        "# import openai\n",
        "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "# embeddings = OpenAIEmbeddings(model_name=\"ada\")\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s2p3rUwvOvW",
        "outputId": "1a027ca6-52a0-43fc-a720-84889b61429c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "query_result = embeddings.embed_query(\"Hello world\")\n",
        "len(query_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXhIY5SrrRec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ffa956e-a800-44b9-ad4e-585075e1d0ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m225.3/244.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pinecone-client -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vySq5oI5sU5V"
      },
      "source": [
        "https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfIpYLV-acks",
        "outputId": "cc8b22e7-3401-4b10-a2e2-9a4ea4ca8928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully connected to the index: newdata\n",
            "Successfully created or connected to the Langchain index: <langchain_community.vectorstores.pinecone.Pinecone object at 0x7ce1b7e432e0>\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "import time\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
        "\n",
        "#074e0d9a-ab5e-48bf-8eae-9effae335521              This is the API to MYDB Insert this\n",
        "\n",
        "# Prompt for Pinecone API key if not set in the environment\n",
        "if not os.getenv(\"PINECONE_API_KEY\"):\n",
        "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
        "\n",
        "# Retrieve the Pinecone API key from environment variables\n",
        "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "# Define your index name\n",
        "index_name = \"newdata\"  # Change if desired\n",
        "\n",
        "# Check for existing indexes\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "# Create the index if it does not exist\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,  # Adjust this to match your embeddings' dimension\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "\n",
        "    # Wait until the index is ready\n",
        "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "        print(\"Waiting for the index to be ready...\")\n",
        "        time.sleep(1)\n",
        "\n",
        "# Connect to the index\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Print connection success message\n",
        "print(f\"Successfully connected to the index: {index_name}\")\n",
        "\n",
        "\n",
        "# Now create a Pinecone index for Langchain using the existing index\n",
        "langchain_index = LangchainPinecone.from_existing_index(\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings\n",
        "\n",
        ")\n",
        "\n",
        "# Output to verify the index creation\n",
        "print(f\"Successfully created or connected to the Langchain index: {langchain_index}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5r7YLpbchAD",
        "outputId": "af06e4d9-e7bf-4878-b0b9-4831796b79dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conservative phase locking, requiresthateverytransaction lockalltheitemsit needsin    1read_lock  read_item  readJock read_item writeJock writeJock  FIGURE18.5Illustrating thedeadlock problem.  partial schedule  .   thatisin  stateof deadlock. Awaitforgraph   partial schedule  . .These protocols   generally   practice, either because  unrealistic assumptionsor\n",
            "schedules thatis,someserializable schedules  beprohibited bytheprotocol.  addition, theuse oflockscancausetwoadditional problems deadlock andstarvation.  discusstheseproblems andtheirsolutions inthenextsection. ..3DealingwithDeadlock  Starvation Deadlock occurswheneachtransaction   setoftwoormoretransactions iswaitingfor someitemthatislockedbysomeothertransaction intheset.Hence,eachtransaction     onawaitingqueue,waitingforoneoftheothertransactions intheset \n",
            "becauseoftheir possibleoverhead. Deadlock detection  timeouts seebelow morepractical.\n",
            "deadlock, sinceT waits  onlyifTS. Thomas WriteRule. Amodification ofthebasicTOalgorithm, knownas Thomas writerule,doesnotenforceconflictserializability butitrejectsfewerwrite operations, bymodifying thechecksforthewrite_itemoperation  follows . Ifread_TS,thenabortandrollbackTandrejecttheoperation. .Ifwrite_TS,thendonotexecutethewriteoperation butcontinue processing. Thisisbecause sometransaction withtimestamp greaterthan\n",
            "Eswaranet . .Bernstein  .,GrayandReuter,andPapadimi trioufocusonconcurrency controlandrecovery. Kumarfocusesonperfor mance  concurrency controlmethods. Locking  discussed    . , andWeinberger ,KedemandSilbershatz ,andKorth.Deadlocks andwaitforgraphswereformalized byHolt,andthewaitwound andwound schemesare presented inRosenkrantz  ..Cautious waitingis discussed  \n",
            "releasethelockonanitem.Asimpleexample isshownin Figure .,wherethetwo transactions andTzaredeadlocked  apartialschedule isonthewaitingqueue ,whichislockedbyT2whileT2isonthewaitingqueueforY,whichislockedby .Meanwhile, neitherTInorTznoranyothertransaction canaccessitemsXandY. Deadlock Prevention Protocols. Oneway  prevent deadlock istouse  deadlock prevention protocol. Onedeadlock prevention protocol, whichis  \n",
            "instead, waitsuntilalltheitems  available forlocking. Conservative 2PLisa deadlock protocol,  weshallsee ..3whenwe discuss thedeadlock problem. However,   difficult touse inpractice because  theneedtopredeclare  setandwrite, whichisnotpossible   situations. Inpractice, themostpopularvariation of2PLisstrict2PL,whichguarantees strict schedules ..Inthisvariation, atransaction  doesnotreleaseany ofits\n",
            "mademanychanges. Another simpleschemetodealwithdeadlock istheuse oftimeouts, Thismethodis practicalbecauseof   overhead andsimplicity. Inthismethod,  atransaction waits foraperiodlongerthanasystemdefined timeout period,thesystem assumes thatthe transaction   deadlocked andabortsitregardless ofwhether adeadlock actually existsornot. Starvation. Another problem thatmayoccurwhenwe uselockingisstarvation, whichoccurswhenatransaction cannotproceed  anindefinite periodoftimewhile\n",
            "Selected BibliographyI995 Nijssen,.,.  Modelling inDataBaseManagement Systems, NorthHolland, . Nijssen,.,.  Architecture andModels  DataBaseManagement Systems, NorthHolland, . Nwosu, ., Berra, ., Thuraisingham, ., . , DesignandImplementation  Multimedia Database Management Systems, KluwerAcademic, . Obermarck, .  Distributed Deadlock Detection Algorithms, ,,  .\n",
            " unlock ifLOCKwriteIocked thenbeginLOCK ..unlocked wakeuponeofthewaitingtransactions, ifany  elseifLOCKlocked thenbegin no_oUeads no_oUeads  ifno_oUeacts thenbeginLOCK unlocked wakeuponeofthewaitingtransactions, ifany   FIGURE18.2Lockingandunlocking operations fortwo write  sharedexclusive locks. . Atransaction  willnotissue awrite_lockoperation  italreadyholdsa\n",
            "itemthatdescribes thestatusoftheitemwithrespectto possible operations thatcan  appliedto .Generally, thereisonelock foreachdataiteminthedatabase. Locks   asameansofsynchronizing theaccess  concurrent transactions tothedatabase items.  ..  discuss thenatureandtypesoflocks. ,.., wepresentprotocols thatuselockingtoguarantee serializability oftransaction schedules. Finally,  ..  discuss  problems associated withtheuse oflocks\n",
            "considered indivisible nointerleaving shouldbeallowedonceoneoftheoperations  starteduntileithertheoperation terminates bygranting thelockorthetransaction  placedonawaitingqueuefortheitem. Whenwe usethesharedexclusive locking scheme, thesystemmustenforcethe following rules .Atransaction Tmustissuetheoperation read_lock  write_lockbefore anyread_itemoperation isperformed . . Atransaction Tmustissuetheoperation write_lockbeforeany write_ operation isperformed .\n",
            "algorithm,  atransaction isunabletoobtaina ,   immediately abortedand  restarted afteracertaintimedelaywithout checking whether adeadlock  actually occurornot.Because thisschemecancausetransactions toabortandrestartneedlessly, thecautious waitingalgorithm wasproposed    reducethenumber  needless abortsrestarts. Suppose thattransaction Tjtries    itemXbutisnotable    becauseX islockedby  othertransaction Tjwithaconflicting .Thecautious\n",
            "Distributed concurrency controltechniques basedonlockinganddistinguished copiesare presented byMenasce .andMinoura andWiederhold .Obermark presents algorithms fordistributed deadlock detection.  survey  recovery techniques indistributed systems  givenbyKohler. discusses atomicactionsondistributed .AbookeditedbyBhargava presents variousapproaches andtechniques forconcurrency  reliability  distributed systems.\n",
            "onallblockedtransactions, sonocyclethatcausesdeadlock canoccur. Deadlock Detection andTimeouts. Asecond practicalapproach  dealingwithdeadlock isdeadlock detection, wherethesystemchecksif astateof deadlock actually exists.Thissolution isattractive  weknowtherewill  little interference amongthetransactions , ifdifferent transactions  rarely access thesame itemsatthesametime.Thiscanhappenifthetransactions areshortandeach\n",
            " orwriteanitemunlessthetransaction thatlastwrotetheitemhascommitted  abortedandrolledback.However, deadlocks canoccurinstricttwophase locking, \n",
            "submitted tothesystem, atimestamp canbethought  asthetransaction starttime.   refer  thetimestamp oftransaction  .Concurrency controltechniques basedontimestamp ordering donotuse locks hence,deadlocks cannotoccur. Timestamps canbegenerated inseveralways.Onepossibility     counterthat isincremented eachtimeitsvalueisassigned  atransaction. Thetransaction timestamps arenumbered ,, ,...inthisscheme. Acomputer counter   finite\n",
            "according tothatorder.Thisrequiresthattheprogrammer orthesystem  aware ofthe chosenorderoftheitems,whichis alsonotpractical inthedatabase context. Anumber ofotherdeadlock prevention schemes havebeenproposed thatmake  decision aboutwhatto dowithatransaction involved  apossible deadlock situation Shouldit beblockedandmadetowaitorshouldit beaborted, orshouldthetransaction preempt andabortanothertransaction? Thesetechniques usetheconcept oftransaction\n",
            "namely,deadlock andstarvation   theseproblems arehandled. ..1Types  Locks  System  Tables Severaltypes  locks    concurrency control.Tointroduce lockingconcepts  ually,  first discuss binarylocks,whichare simple  restrictive    notused  practice. Wethendiscusssharedexclusive locks,whichprovidemoregenerallockingcapa bilitiesandare   practical database lockingschemes.  ..,  describe \n",
            ".Phase LockingTechniques forConcurrency ControlI587 readJock ifLOCKunlocked thenbeginLOCKlocked no_oCreacts  elseifLOCKIocked thenno_oCreacts no_of_reacts  elsebeginwait untilLOCKunlocked  thelocimanagerwakesupthetransaction gotoS  writeJock ifLOCKunlocked thenLOCKwritelocked elsebegin untilLOCKunlocl  thelockmanagerwakesupthetransaction goto8  unlock\n"
          ]
        }
      ],
      "source": [
        "def get_similar_docs(query, k=20, score=True):\n",
        "    if score:\n",
        "        similar_docs = langchain_index.similarity_search_with_score(query, k=k)  # Use langchain_index\n",
        "    else:\n",
        "        similar_docs = langchain_index.similarity_search(query, k=k)  # Use langchain_index\n",
        "    return similar_docs\n",
        "\n",
        "# Example query\n",
        "query = \"How to avoid deadlocks in os?\"\n",
        "similar_docs = get_similar_docs(query)\n",
        "\n",
        "# Output the similar documents\n",
        "for doc in similar_docs:\n",
        "    print(doc[0].page_content)  # This will print the similar documents found\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVOoc9GXHjOG"
      },
      "source": [
        "How is India's economy?\n",
        "\n",
        "and its culture?\n",
        "\n",
        "its relationship with USA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlkrKhY1PMXm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# ----------------------------------------\n",
        "# Configuration\n",
        "# ----------------------------------------\n",
        "\n",
        "# Set your Hugging Face API key securely\n",
        "# It's recommended to set this as an environment variable for security\n",
        "# For example, in your terminal:\n",
        "# export HUGGINGFACE_API_KEY='your-huggingface-api-key'\n",
        "\n",
        "# Alternatively, you can set it directly in the script (not recommended for production)\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"  # Replace with your actual API key\n",
        "\n",
        "# Hugging Face model details\n",
        "HUGGINGFACE_API_URL = \"https://api-inference.huggingface.co/models/{model}\"\n",
        "  # You can choose any suitable model\n",
        "# For question-answering tasks, models like \"distilbert-base-uncased-distilled-squad\" can be used\n",
        "# Ensure the chosen model supports the desired task\n",
        "\n",
        "# ----------------------------------------\n",
        "# Initialize LangChain Index\n",
        "# ----------------------------------------\n",
        "\n",
        "# Replace the following with your actual LangChain index initialization\n",
        "# Example using FAISS:\n",
        "# from langchain.vectorstores import FAISS\n",
        "# from langchain.embeddings import HuggingFaceEmbedd1111111111111ings\n",
        "#\n",
        "# embeddings = HuggingFaceEmbeddings()\n",
        "# langchain_index = FAISS.load_local(\"path_to_faiss_index\", embeddings)\n",
        "\n",
        "# For demonstration purposes, we'll assume langchain_index is already initialized.\n",
        "# Make sure to replace the following line with your actual index.\n",
        "  # Replace with your LangChain index initialization\n",
        "\n",
        "# ----------------------------------------\n",
        "# Function to Retrieve Similar Documents\n",
        "# ----------------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# Set the Hugging Face API key directly inside the script\n",
        "HUGGINGFACE_API_TOKEN = \"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\"  # Replace with your actual API key\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"  # Replace with your chosen model\n",
        "\n",
        "# Construct the API URL\n",
        "HUGGINGFACE_API_URL = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
        "\n",
        "# Set up the headers with the authorization token\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {HUGGINGFACE_API_TOKEN}\"\n",
        "}\n",
        "\n",
        "# Example query\n",
        "query = \"How to avoid deadlock in OS?\"\n",
        "\n",
        "# Assuming you have a function `get_similar_docs` defined\n",
        "similar_docs = get_similar_docs(query)\n",
        "\n",
        "# Prepare the context from similar_docs\n",
        "context = \"\\n\\n\".join([doc[0].page_content for doc in similar_docs])\n",
        "\n",
        "# Create the prompt\n",
        "prompt = (\n",
        "    f\"Context:\\n{context}\\n\\n\"\n",
        "    f\"Question: {query}\\n\"\n",
        "    f\"Answer:\"\n",
        ")\n",
        "\n",
        "# Function to query Hugging Face Inference API\n",
        "def query_huggingface_api(prompt, max_length=25000):\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": max_length,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"top_k\": 50,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"do_sample\": True,\n",
        "            \"stop\": [\"<|endoftext|>\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(\n",
        "        HUGGINGFACE_API_URL,\n",
        "        headers=headers,\n",
        "        json=payload\n",
        "    )\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        raise Exception(\n",
        "            f\"Request failed with status code {response.status_code}: {response.text}\"\n",
        "        )\n",
        "\n",
        "# Function to extract answer from API response\n",
        "def extract_answer(api_response):\n",
        "    if isinstance(api_response, list) and len(api_response) > 0:\n",
        "        generated_text = api_response[0].get('generated_text', '')\n",
        "        # Assuming the model appends the answer after \"Answer:\"\n",
        "        answer = generated_text.split(\"Answer:\")[-1].strip() if \"Answer:\" in generated_text else generated_text.strip()\n",
        "        if len(answer) > 25000:\n",
        "            answer = answer[:25000] + \"...\"\n",
        "        return answer\n",
        "    else:\n",
        "        return \"No answer generated.\"\n",
        "\n",
        "# Generate the answer using Hugging Face Inference API\n",
        "try:\n",
        "    api_response = query_huggingface_api(prompt, max_length=25000)\n",
        "    answer = extract_answer(api_response)\n",
        "    print(\"Answer:\", answer)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", str(e))\n"
      ],
      "metadata": {
        "id": "sOJfoO9mCih4",
        "outputId": "e0f1c320-6e21-46d4-a29e-95f6f7a4fdae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: To avoid deadlock in Operating System (OS), we can use the following methods:\n",
            "\n",
            "1. Resource Allocation Graph (RAG): It helps in identifying cycles in the system and thus detecting potential deadlocks. Each process is represented as a node in the graph, and each resource is also represented as a node. An edge from a process node to a resource node indicates that the process holds the resource. If there's a cycle in the graph, it means that deadlock might occur.\n",
            "\n",
            "2. Banker's Algorithm: It is used to decide whether a request for resources can be granted without causing a deadlock. The algorithm maintains a safe sequence of processes, which can finish their execution without causing a deadlock. If a new request fits into the safe sequence, the resources are allocated; otherwise, the request is denied.\n",
            "\n",
            "3. Timeout Mechanism: If a process waits for a resource for more than a specific time limit, the process is assumed to be stuck in a deadlock and is killed to resolve the issue.\n",
            "\n",
            "4. Deadlock Avoidance Techniques: These techniques prevent the occurrence of a deadlock instead of resolving it after it has happened. Examples include the resource-ordering technique, where processes must request resources in a specific order, and the banker's algorithm mentioned above.\n",
            "\n",
            "5. Deadlock Prevention Techniques: These techniques ensure that a deadlock never occurs by imposing certain restrictions on the behavior of processes. For example, the \"No Preemption\" rule states that once a process starts using a resource, it won't be taken away until the process is finished with it. The \"No Circular Wait\" rule ensures that no process is waiting for resources held by another process in a circular fashion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BtRV8VeiCkeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "07HrPkf47-Rx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}